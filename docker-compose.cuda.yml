version: '3.8'

services:
  talk3d:
    build:
      context: .
      dockerfile: Docker.cuda.yml
    container_name: talk3d
    runtime: nvidia  # Use NVIDIA runtime to enable GPU access
    environment:
      - PATH=/opt/conda/bin:$PATH
    volumes:
      - .:/app  # Mount current directory to /app in the container
    command: ["/bin/bash"]  # Start with bash
    deploy:
      resources:
        limits:
          cpus: "1.0"     # Limit CPU to 1 core
          memory: 4G      # Limit memory to 4 GB
        reservations:
          cpus: "0.5"       # Reserve 0.5 CPU if needed (you can adjust this)
          memory: 2G        # Reserve 2 GB of memory if needed
          # GPU reservation is usually specified differently
